import threading
import time
from collections import deque
import av
import numpy as np
import streamlit as st
from streamlit_webrtc import WebRtcMode, webrtc_streamer, VideoTransformerBase
import speech_recognition as sr
from typing import List
import io
import wave
import asyncio
import time

import cv2
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_cohere.chat_models import ChatCohere
import base64
import keyboard
from gtts import gTTS
import os

def init_page():
    st.set_page_config(
        page_title="Mr.Yas Chatbot",
        page_icon="ğŸ¤–"
    )
    st.header("Mr.Yas Chatbot ğŸ¤–")
    st.write("""ã‚«ãƒ¡ãƒ©ã¨ãƒã‚¤ã‚¯ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã™ã‚‹ã‚ˆã†ãƒ–ãƒ©ã‚¦ã‚¶ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚
         \nChromeãƒ»ãƒ»ãƒ»support.google.comå‚ç…§ã€‚ã€Œã‚«ãƒ¡ãƒ©ã‚„ãƒã‚¤ã‚¯ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã™ã‚‹ã€ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢,
         \nSafariãƒ»ãƒ»ãƒ»support.apple.comå‚ç…§ã€‚è¨­å®šâ‡’Safariâ‡’ã‚«ãƒ¡ãƒ©åŠã³ãƒã‚¤ã‚¯â‡’ç¢ºèªåˆã¯è¨±å¯,
         \nFirefoxãƒ»ãƒ»ãƒ»support.mozilla.orgå‚ç…§ã€‚         
         """) 
    
    st.sidebar.title("Options")

def init_messages():
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    # clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã‚„ message_history ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã«åˆæœŸåŒ–
    if clear_button or "message_history" not in st.session_state:
        st.session_state.message_history = [
            ("system", "You are a helpful assistant.")
        ]    
def select_model():
    # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã€temperatureã‚’0ã‹ã‚‰2ã¾ã§ã®ç¯„å›²ã§é¸æŠå¯èƒ½ã«ã™ã‚‹
    # åˆæœŸå€¤ã¯0.0ã€åˆ»ã¿å¹…ã¯0.01ã¨ã™ã‚‹
    temperature = st.sidebar.slider(
        "Temperature(å›ç­”ãƒãƒ©ãƒ„ã‚­åº¦åˆ):", min_value=0.0, max_value=2.0, value=0.0, step=0.01)
    models = ( "GPT-4o", "Claude 3.5 Sonnet", "Gemini 1.5 Pro")
    model = st.sidebar.radio("Choose a modelï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼‰:", models)

       
    if model == "GPT-4o":  #"gpt-4o 'gpt-4o-2024-08-06'" æœ‰æ–™ï¼Ÿã€Best
        st.session_state.model_name = "gpt-4o"
        return ChatOpenAI(
            temperature=temperature,
            model=st.session_state.model_name,
            api_key= st.secrets.key.OPENAI_API_KEY,
            max_tokens=512,  #æŒ‡å®šã—ãªã„ã¨çŸ­ã„å›ç­”ã«ãªã£ãŸã‚Šã€é€”åˆ‡ã‚ŒãŸã‚Šã™ã‚‹ã€‚
            streaming=True,
        )
    elif model == "Claude 3.5 Sonnet": #ã‚³ãƒ¼ãƒ‰ãŒGoodï¼ï¼
        st.session_state.model_name = "claude-3-5-sonnet-20240620"
        return ChatAnthropic(
            temperature=temperature,
            #model=st.session_state.model_name,
            model_name=st.session_state.model_name, 
            api_key= st.secrets.key.ANTHROPIC_API_KEY,
            max_tokens_to_sample=2048,  
            timeout=None,  
            max_retries=2,
            stop=None,  
        )
    elif model == "Gemini 1.5 Pro":
        st.session_state.model_name = "gemini-1.5-pro-latest"
        return ChatGoogleGenerativeAI(
            temperature=temperature,
            model=st.session_state.model_name,
            api_key= st.secrets.key.GOOGLE_API_KEY,
        )
#éŸ³å£°å‡ºåŠ›é–¢æ•°
def speak(text):
    #st.write("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚")
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›
    tts = gTTS(text=text, lang='ja')
    output_file = "output.mp3"
    tts.save(output_file)
    st.write("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æä¾›
    audio_file = open(output_file, "rb")
    audio_bytes = audio_file.read()
    st.audio(audio_bytes, format="audio/mp3", start_time=0,autoplay=True)
    #st.write("éŸ³å£°å†ç”ŸãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    audio_file.close()
    os.remove(output_file)
    #st.write("éŸ³å£°å†ç”ŸãŒå®Œäº†ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸã€‚")

#  LLMå•ç­”é–¢æ•°   
async def query_llm(user_input,frame):
    print("user_input=",user_input)
    
    try:
            
        # ç”»åƒã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›ï¼ˆä¾‹ï¼šbase64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãªã©ï¼‰
        # ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        encoded_image = cv2.imencode('.jpg', frame)[1]
        # ç”»åƒã‚’Base64ã«å¤‰æ›
        base64_image = base64.b64encode(encoded_image).decode('utf-8')  
        #image = f"data:image/jpeg;base64,{base64_image}"
        
        if st.session_state.model_name ==  "keep_gpt-4o":
            llm = st.session_state.llm  
            stream = llm.stream([
                    *st.session_state.message_history,
                    (
                        "user",
                        [
                            {
                                "type": "text",
                                "text": user_input
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "auto"
                                },
                            }
                        ]
                    )
                ])
            #response = chain.invoke(user_input)
            # LLMã®è¿”ç­”ã‚’è¡¨ç¤ºã™ã‚‹  Streaming
            with st.chat_message('ai'):   
                #st.write(response)  
                response = st.write_stream(stream) 
            #full = next(stream)
            #for chunk in stream:
                #full += chunk
            #response = full    
            print(response)
          
        
        if st.session_state.model_name ==  "command-r-plus":
            print("st.session_state.model_name=",st.session_state.model_name)
            print(user_input)
            prompt = ChatPromptTemplate.from_messages(
                [
                    *st.session_state.message_history,
                     #("user", f"{user_input}:{base64_image}"),  #ã‚„ã£ã±ã‚Šã ã‚
                     ("user", f"{user_input}")
                ]
            )
            
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            #stream = chain.stream(user_input,base64_image)
            
            stream = chain.stream({"user_input":user_input,"base64_image": base64_image})
            print("stream=",stream)
            #response = chain.invoke(user_input)
            # LLMã®è¿”ç­”ã‚’è¡¨ç¤ºã™ã‚‹  Streaming
            with st.chat_message('ai'):   
                #st.write(response)  
                response =st.write_stream(stream) 
            print("response=",response)

        elif st.session_state.model_name ==  "keep_command-r-plus":
            print("st.session_state.model_name=",st.session_state.model_name)
            prompt = ChatPromptTemplate.from_messages([
                    *st.session_state.message_history,
                    ("user", "{user_input}")  # ã“ã“ã«ã‚ã¨ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãŒå…¥ã‚‹
                ])
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            stream = chain.stream(user_input)
            
            #response = chain.invoke(user_input)
            # LLMã®è¿”ç­”ã‚’è¡¨ç¤ºã™ã‚‹  Streaming
            with st.chat_message('ai'):   
                #st.write(response)  
                response =st.write_stream(stream) 
            print("response=",response)
        else:
            print("st.session_state.model_name=",st.session_state.model_name)
            prompt = ChatPromptTemplate.from_messages(
                [
                    *st.session_state.message_history,
                    (
                        "user",
                        [
                            {
                                "type": "text",
                                "text": user_input
                            },
                            {
                                "type": "image_url",
                                "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                            }
                        ],
                    ),
                ]
            )
             
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            #stream = chain.stream(user_input,base64_image)
            stream = chain.stream({"user_input":user_input,"base64_image": base64_image})
            #print("stream=",stream)
            #response = chain.invoke(user_input)
            # LLMã®è¿”ç­”ã‚’è¡¨ç¤ºã™ã‚‹  Streaming
            with st.chat_message('ai'):   
                #st.write(response)  
                response =st.write_stream(stream) 
            #print("response=",response)            
        

        print(f"{st.session_state.model_name}=",response)
        

        # éŸ³å£°å‡ºåŠ›å‡¦ç†                
        if st.session_state.output_method == "éŸ³å£°":
            #st.write("éŸ³å£°å‡ºåŠ›ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            speak(response)   #st.audio ok
            #speak1(response) pygame NG
            #speak_thread = speak_async(response)
            # å¿…è¦ã«å¿œã˜ã¦éŸ³å£°åˆæˆã®å®Œäº†ã‚’å¾…ã¤
            #speak_thread.join() 
            #print("éŸ³å£°å†ç”ŸãŒå®Œäº†ã—ã¾ã—ãŸã€‚æ¬¡ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
            #st.write("éŸ³å£°å†ç”ŸãŒå®Œäº†ã—ã¾ã—ãŸã€‚æ¬¡ã®å‡¦ç†ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚")
            
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
        st.session_state.message_history.append(("user", user_input))
        st.session_state.message_history.append(("ai", response))
    
        return response
    except StopIteration:
        # StopIterationã®å‡¦ç†
        print("StopIterationãŒç™ºç”Ÿ")
        pass

    user_input = ""
    base64_image = ""
    frame = ""   

def main():
    #st.header("Real Time Speech-to-Text with_video")
    #ç”»é¢è¡¨ç¤º
    init_page()
    init_messages()
    #stã§ä½¿ã†å¤‰æ•°åˆæœŸè¨­å®š
    st.session_state.llm = select_model()
    st.session_state.input_method = ""
    st.session_state.user_input = ""
    st.session_state.result = ""
    st.session_state.frame = "" 
    col, col2 = st.sidebar.columns(2)
     # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
    with col:
        # å…¥åŠ›æ–¹æ³•ã®é¸æŠ
        input_method = st.sidebar.radio("å…¥åŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.input_method = input_method
     # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
    with col2:
        # å‡ºåŠ›æ–¹æ³•ã®é¸æŠ
        output_method = st.sidebar.radio("å‡ºåŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.output_method = output_method
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º (ç¬¬2ç« ã‹ã‚‰å°‘ã—ä½ç½®ãŒå¤‰æ›´ã«ãªã£ã¦ã„ã‚‹ã®ã§æ³¨æ„)
    for role, message in st.session_state.get("message_history", []):
        st.chat_message(role).markdown(message)
    #ãƒ‡ãƒ¼ã‚¿åˆæœŸå€¤
    user_input = ""
    base64_image = ""
    frame = ""    
    app_sst_with_video() 

class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        self.frame = None

    def recv(self, frame):   
        self.frame = frame.to_ndarray(format="bgr24")
        return frame
async def process_audio(audio_data_bytes, sample_rate, text_output):
    audio_data_io = io.BytesIO()
    with wave.open(audio_data_io, 'wb') as wf:
        wf.setnchannels(2)
        #wf.setnchannels(1)  # ãƒ¢ãƒãƒ©ãƒ«éŸ³å£°ã¨ã—ã¦è¨˜éŒ²ã€€NG
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio_data_bytes)
    audio_data_io.seek(0)

    recognizer = sr.Recognizer()
    with sr.AudioFile(audio_data_io) as source:
        audio_data = recognizer.record(source)
        try:
            text = recognizer.recognize_google(audio_data, language="ja-JP")
            print("èªè­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:", text)
            text_output.write(f"å¤‰æ›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆï¼š{text}")  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
            st.session_state.user_input = text
        except sr.UnknownValueError:
            text = "éŸ³å£°ã‚’èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            #text_output.write(f"å¤‰æ›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆï¼š{text}")  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã«ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        except sr.RequestError as e:
            text = f"ã‚µãƒ¼ãƒ“ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸ; {e}"
            #text_output.write(f"å¤‰æ›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆï¼š{text}") 
    return text
def app_sst_with_video():
    text_input = ""
    frames_deque_lock = threading.Lock()
    frames_deque: deque = deque([])

    async def queued_audio_frames_callback(
        frames: List[av.AudioFrame],
    ) -> av.AudioFrame:
        with frames_deque_lock:
            frames_deque.extend(frames)

        # Return empty frames to be silent.
        new_frames = []
        for frame in frames:
            input_array = frame.to_ndarray()
            new_frame = av.AudioFrame.from_ndarray(
                np.zeros(input_array.shape, dtype=input_array.dtype),
                layout=frame.layout.name,
            )
            new_frame.sample_rate = frame.sample_rate
            new_frames.append(new_frame)

        return new_frames
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¡¨ç¤º
    with st.sidebar:
        st.header("Webcam Stream")
        webrtc_ctx = webrtc_streamer(
            key="speech-to-text-w-video",
            desired_playing_state=True, 
            mode=WebRtcMode.SENDRECV, #.SENDONLY,  #
            #audio_receiver_size=2048,  #1024ã€€#512 #ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4
            #å°ã•ã„ã¨Queue overflow. Consider to set receiver size bigger. Current size is 1024.
            queued_audio_frames_callback=queued_audio_frames_callback,
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            media_stream_constraints={"video": True, "audio": True},
            video_processor_factory=VideoTransformer,  #æ©Ÿèƒ½ã—ã¦ã„ã‚‹ï¼Ÿ
        )

    
    
    if not webrtc_ctx.state.playing:
        return
    #status_indicator.write("Loading...")

    ###################################################################
    #éŸ³å£°å…¥åŠ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ãŸå…¥åŠ›ï¼‰ã®å¯¾è©±ãƒ«ãƒ¼ãƒ—
    #print("Before_st.session_state.input_method=",st.session_state.input_method)
    if st.session_state.input_method == "éŸ³å£°": 
        
        status_indicator = st.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        status_indicator.write("Loading...")
        text_output = st.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        
        st.sidebar.header("Capture Image")
        cap_image = st.sidebar.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        
        while True:
            if webrtc_ctx.state.playing:
                audio_frames = []
                with frames_deque_lock:
                    while len(frames_deque) > 0:
                        frame = frames_deque.popleft()
                        audio_frames.append(frame)

                if len(audio_frames) == 0:
                    time.sleep(0.1)
                    status_indicator.write("No frame arrived.")
                    continue

                status_indicator.write("ğŸ¤—ä½•ã‹è©±ã—ã¦!")
                audio_buffer = []  # ãƒãƒƒãƒ•ã‚¡ã‚’åˆæœŸåŒ–
                for audio_frame in audio_frames:
                    
                    # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ numpy é…åˆ—ã¨ã—ã¦å–å¾—ï¼ˆs16 ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ int16 ã¨ã—ã¦è§£é‡ˆï¼‰
                    audio = audio_frame.to_ndarray().astype(np.int16)
                    audio_buffer.append(audio)  # ãƒãƒƒãƒ•ã‚¡ã«ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 

                    # æ­£è¦åŒ–ã—ã¦ -1.0 ã‹ã‚‰ 1.0 ã®ç¯„å›²ã«åã‚ã‚‹
                    #max_val = np.max(np.abs(audio_buffer))
                    #if max_val > 0:
                        #audio_buffer = audio_buffer / max_val

                if len(audio_buffer) >0:  # 100: # 
                    # è¤‡æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã¾ã¨ã‚ã‚‹
                    audio_data = np.concatenate(audio_buffer)
                    audio_data_bytes= audio_data.tobytes()
                    st.session_state.user_input=""
                    # éåŒæœŸã§éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
                    text_input=asyncio.run(process_audio(audio_data_bytes, frame.sample_rate, text_output))
                    # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
                    audio_buffer.clear() 
                    #print("ã“ã“ã‚’é€šéE2") #ã“ã“ã¾ã§OK
                    #print("text_input=",text_input)
                    if st.session_state.user_input !="":
                        print("st.session_state.user_input=",st.session_state.user_input)  
                        #llm_in()
                        print("user_input=",st.session_state.user_input)
                        with text_output.chat_message('user'):   
                            st.write(st.session_state.user_input) 
                        # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
                        #ç¾åœ¨ã®ç”»åƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹
                        cap = None    
                        #ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ¼ç”»åƒå…¥åŠ›
                        if webrtc_ctx.video_transformer:  
                            cap = webrtc_ctx.video_transformer.frame
                        if cap is not None and st.session_state.user_input !="":
                            #st.sidebar.header("Capture Image")
                            cap_image.image(cap, channels="BGR")
                            # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
                            with st.spinner("Querying LLM..."):
                                loop = asyncio.new_event_loop()
                                asyncio.set_event_loop(loop)
                                st.session_state.result= ""
                                result = loop.run_until_complete(query_llm(st.session_state.user_input,cap))
                                st.session_state.result = result
                                result = ""
                                #result = await query_llm(text,frame)
                                st.session_state.user_input=""
                                        
            else:
                status_indicator.write("Stopped.")
                break

    ################################################################### 
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®å ´åˆ
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    if st.session_state.input_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
        

        button_input = ""
        # 4ã¤ã®åˆ—ã‚’ä½œæˆ
        col1, col2, col3, col4 = st.columns(4)
        # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
        with col1:
            if st.button("ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
                button_input = "ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"
        with col2:
            if st.button("å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"):
                button_input = "å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"
        with col3:
            if st.button("ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"):
                button_input = "ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"
        with col4:
            if st.button("äººç”Ÿã®æ„ç¾©ã¯ï¼Ÿ"):
                button_input = "äººç”Ÿã®æ„ç¾©ï¼Ÿ"
        col5, col6, col7, col8 = st.columns(4)
        with col5:
            if st.button("æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚"):
                button_input = "æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚"
        with col6:
            if st.button("å–„æ‚ªã¯ä½•ã§æ±ºã¾ã‚Šã¾ã™ã‹ï¼Ÿ"):
                button_input = "å–„æ‚ªã¯ä½•ã§æ±ºã¾ã‚Šã¾ã™ã‹ï¼Ÿ"
        with col7:
            if st.button("æ—¥æœ¬ã®è¦³å…‰åœ°ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"):
                button_input = "æ—¥æœ¬ã®è¦³å…‰åœ°ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
        with col8:
            if st.button("ä»Šæ—¥ã®æ–™ç†ã¯ãªã«ãŒã„ã„ã‹ãª"):
                button_input = "ä»Šæ—¥ã®æ–™ç†ã¯ãªã«ãŒã„ã„ã‹ãª"
        if button_input !="":
            st.session_state.user_input=button_input

        text_input =st.chat_input("ğŸ¤—ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ã“ã“ã«å…¥åŠ›ã—ã¦ã­ï¼") #,key=st.session_state.text_input)
        #text_input = st.text_input("ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å…¥åŠ›ã—ã¦ãã ã•ã„:", key=st.session_state.text_input) 
        if text_input:
            st.session_state.user_input=text_input
            text_input=""
            #llm_in()
            print("user_input=",st.session_state.user_input)
        with st.chat_message('user'):   
            st.write(st.session_state.user_input) 
        # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
        #ç¾åœ¨ã®ç”»åƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹
        cap = None    
        #ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ¼ç”»åƒå…¥åŠ›
        if webrtc_ctx.video_transformer:  
            cap = webrtc_ctx.video_transformer.frame
        if cap is not None and st.session_state.user_input !="":
            st.sidebar.header("Capture Image")
            st.sidebar.image(cap, channels="BGR")
            # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
            with st.spinner("Querying LLM..."):
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                st.session_state.result= ""
                result = loop.run_until_complete(query_llm(st.session_state.user_input,cap))
                st.session_state.result = result
                result = ""
                #result = await query_llm(text,frame)
                st.session_state.user_input=""

################################################################### 
 
###################################################################      
if __name__ == "__main__":
    
    main()

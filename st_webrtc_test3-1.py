import threading
import time
from collections import deque
import av
import numpy as np
import streamlit as st
from streamlit_webrtc import WebRtcMode, webrtc_streamer, VideoTransformerBase
from typing import List
import io
import wave
import asyncio
import cv2
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_cohere.chat_models import ChatCohere
import base64
import keyboard
from gtts import gTTS
import os
#import speech_recognition as sr
from pydub import AudioSegment
import queue, pydub, tempfile
import whisper
import torch
import torchaudio
import torchvision
import re
from pydub.effects import low_pass_filter, high_pass_filter
from io import BytesIO

def init_page():
    st.set_page_config(
        page_title="Yas Chatbot(Webã‚«ãƒ¡ã®ç”»åƒã€éŸ³å£°ã‚’è¡¨ç¤º)",
        page_icon="ğŸ¤–"
    )
    st.header("Yas Chatbot ğŸ¤–")
    st.write("""Webã‚«ãƒ¡ãƒ©ã«ç§»ã—ãŸç”»åƒã«ã¤ã„ã¦ã®å•åˆã›ã€éŸ³å£°ã§ã®å…¥å‡ºåŠ›ãŒã§ãã¾ã™ã€‚\n
             Webãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚«ãƒ¡ãƒ©,ãƒã‚¤ã‚¯ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã™ã‚‹è¨­å®šã«ã—ã¦ãã ã•ã„ã€‚""") 
    st.sidebar.title("Options")

def init_messages():
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    # clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã‚„ message_history ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã«åˆæœŸåŒ–
    if clear_button or "message_history" not in st.session_state:
        st.session_state.message_history = [
            ("system", "You are a helpful assistant.")
        ]    
def select_model():
    # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã€temperatureã‚’0ã‹ã‚‰2ã¾ã§ã®ç¯„å›²ã§é¸æŠå¯èƒ½ã«ã™ã‚‹
    # åˆæœŸå€¤ã¯0.0ã€åˆ»ã¿å¹…ã¯0.01ã¨ã™ã‚‹
    temperature = 0.0
    models = ( "GPT-4o", "Claude 3.5 Sonnet", "Gemini 1.5 Pro")
    model = st.sidebar.radio("å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ:", models)
       
    if model == "GPT-4o":  #"gpt-4o 'gpt-4o-2024-08-06'" æœ‰æ–™ï¼Ÿã€Best
        st.session_state.model_name = "gpt-4o"
        return ChatOpenAI(
            temperature=temperature,
            model=st.session_state.model_name,
            api_key= st.secrets.key.OPENAI_API_KEY,
            max_tokens=512,  #æŒ‡å®šã—ãªã„ã¨çŸ­ã„å›ç­”ã«ãªã£ãŸã‚Šã€é€”åˆ‡ã‚ŒãŸã‚Šã™ã‚‹ã€‚
            streaming=True,
        )
    elif model == "Claude 3.5 Sonnet": #ã‚³ãƒ¼ãƒ‰ãŒGoodï¼ï¼
        st.session_state.model_name = "claude-3-5-sonnet-20240620"
        return ChatAnthropic(
            temperature=temperature,
            model_name=st.session_state.model_name, 
            api_key= st.secrets.key.ANTHROPIC_API_KEY,
            max_tokens_to_sample=2048,  
            timeout=None,  
            max_retries=2,
            stop=None,  
        )
    elif model == "Gemini 1.5 Pro":
        st.session_state.model_name = "gemini-1.5-pro-latest"
        return ChatGoogleGenerativeAI(
            temperature=temperature,
            model=st.session_state.model_name,
            api_key= st.secrets.key.GOOGLE_API_KEY,
        )
#éŸ³å£°å‡ºåŠ›é–¢æ•°
def streaming_text_speak(llm_response):
    # æœ«å°¾ã®ç©ºç™½ã®æ•°ã‚’ç¢ºèª
    trailing_spaces = len(llm_response) - len(llm_response.rstrip())
    print(f"æœ«å°¾ã®ç©ºç™½ã®æ•°: {trailing_spaces}")
    # æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    cleaned_response = llm_response.rstrip()
    print(f"ç©ºç™½ã‚’é™¤å»ã—ãŸæ–‡å­—åˆ—: '{cleaned_response}'")
    # å¥èª­ç‚¹ã‚„ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŸºæº–ã«åˆ†å‰²
    #å¾©å¸°æ–‡å­—ï¼ˆ\rï¼‰ã¯ã€**ã‚­ãƒ£ãƒªãƒƒã‚¸ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆCarriage Returnï¼‰**ã¨å‘¼ã°ã‚Œã‚‹ç‰¹æ®Šæ–‡å­—ã§ã€
    # ASCIIã‚³ãƒ¼ãƒ‰13ï¼ˆ10é€²æ•°ï¼‰ã«å¯¾å¿œã—ã¾ã™ã€‚ä¸»ã«æ”¹è¡Œã®ä¸€éƒ¨ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹åˆ¶å¾¡æ–‡å­—ã§ã™ã€‚
    split_response = re.split(r'([\r\n!-;=:ã€ã€‚ \?]+)', llm_response) 
    #split_response = re.split(r'([;:ã€ã€‚ ]+ğŸ˜ŠğŸŒŸğŸš€ğŸ‰)', llm_response)  #?ã¯ãªãã¦ã‚‚OK
    split_response = [segment for segment in split_response if segment.strip()]  # ç©ºè¦ç´ ã‚’å‰Šé™¤
    print(split_response)
    # AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
    with st.chat_message("ai"):
        response_placeholder = st.empty()
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã¨éŸ³å£°å‡ºåŠ›å‡¦ç†
        partial_text = ""
        
        for segment in split_response:
            if segment.strip():  # ç©ºæ–‡å­—åˆ—ã§ãªã„å ´åˆã®ã¿å‡¦ç†
                partial_text += segment
                response_placeholder.markdown(f"**{partial_text}**")  # å¿œç­”ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
                # gTTSã§éŸ³å£°ç”Ÿæˆï¼ˆéƒ¨åˆ†ãƒ†ã‚­ã‚¹ãƒˆï¼‰
                try:
                    # ã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯ã‚„ãã®ä»–ã®ç™ºéŸ³ã«ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
                    cleaned_segment = re.sub(r'[\*!-]', '', segment)
                    tts = gTTS(cleaned_segment, lang="ja")  # éŸ³å£°åŒ–
                    audio_buffer = BytesIO()
                    tts.write_to_fp(audio_buffer)  # ãƒãƒƒãƒ•ã‚¡ã«æ›¸ãè¾¼ã¿
                    audio_buffer.seek(0)

                    # pydubã§å†ç”Ÿé€Ÿåº¦ã‚’å¤‰æ›´
                    audio = AudioSegment.from_file(audio_buffer, format="mp3")
                    audio = audio._spawn(audio.raw_data, overrides={
                        "frame_rate": int(audio.frame_rate * 1.3)  # 1.5å€é€Ÿ
                    }).set_frame_rate(audio.frame_rate)
                    audio_buffer.close()

                    # éŸ³è³ªèª¿æ•´
                    audio = audio.set_frame_rate(44100)  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
                    audio = audio + 5  # éŸ³é‡ã‚’5dBå¢—åŠ 
                    audio = audio.fade_in(500).fade_out(500)  # ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ã‚¢ã‚¦ãƒˆ
                    #audio = audio.low_pass_filter(3000)  # é«˜éŸ³åŸŸã‚’ã‚«ãƒƒãƒˆ
                    audio = low_pass_filter(audio, cutoff=900)  # é«˜éŸ³åŸŸã‚’ã‚«ãƒƒãƒˆ
                    # ãƒ™ãƒ¼ã‚¹ãƒ–ãƒ¼ã‚¹ãƒˆï¼ˆä½éŸ³åŸŸã‚’å¼·èª¿ï¼‰
                    low_boost = low_pass_filter(audio,1000).apply_gain(10)
                    audio = audio.overlay(low_boost)

                    # ãƒãƒƒãƒ•ã‚¡ã«å†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                    output_buffer = BytesIO()
                    audio.export(output_buffer, format="mp3")
                    output_buffer.seek(0)

                    # éŸ³å£°ã®å†ç”Ÿ
                    # ãƒã‚§ãƒƒã‚¯ã™ã‚‹æ–‡å­—åˆ—
                    if re.search(r"\n\n", segment):
                        print("æ–‡å­—åˆ—ã« '\\n\\n' ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
                        #time.sleep(1) 
                    #else:
                        #print("æ–‡å­—åˆ—ã« '\\n\\n' ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                    #st.audio(audio_buffer, format="audio/mp3",autoplay = True)
                    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’Base64ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                    audio_base64 = base64.b64encode(output_buffer.read()).decode()
                    audio_buffer.close()  # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒ­ãƒ¼ã‚º
                    a=len(audio_base64)
                    #print(a)
                     # HTMLã‚¿ã‚°ã§éŸ³å£°ã‚’è‡ªå‹•å†ç”Ÿï¼ˆãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼éè¡¨ç¤ºã€å†ç”Ÿé€Ÿåº¦èª¿æ•´ï¼‰
                    audio_html = f"""
                   <audio id="audio-player" autoplay style="display:none;">
                        <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
                    </audio>
                    """
                    st.markdown(audio_html, unsafe_allow_html=True)

                except Exception as e:
                    #print(f"éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                    pass
                try:
                    time.sleep(a*0.00004)  # ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€Ÿåº¦ã«åŒæœŸ
                except Exception as e:
                  time.sleep(2) 
                    
def speak(text):
    #st.write("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚")
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›
    tts = gTTS(text=text, lang='ja')
    output_file = "output.mp3"
    tts.save(output_file)
    st.write("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æä¾›
    audio_file = open(output_file, "rb")
    audio_bytes = audio_file.read()
    st.audio(audio_bytes, format="audio/mp3", start_time=0,autoplay=True)
    #st.write("éŸ³å£°å†ç”ŸãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    audio_file.close()
    os.remove(output_file)
    #st.write("éŸ³å£°å†ç”ŸãŒå®Œäº†ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸã€‚")

#  LLMå•ç­”é–¢æ•°   
async def query_llm(user_input,frame):
    try:
        if st.session_state.input_img == "æœ‰":    
            # ç”»åƒã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›ï¼ˆä¾‹ï¼šbase64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãªã©ï¼‰
            # ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            encoded_image = cv2.imencode('.jpg', frame)[1]
            # ç”»åƒã‚’Base64ã«å¤‰æ›
            base64_image = base64.b64encode(encoded_image).decode('utf-8')  
            #image = f"data:image/jpeg;base64,{base64_image}"
        
        if st.session_state.model_name ==  "keep_gpt-4o":
            llm = st.session_state.llm  
            stream = llm.stream([
                    *st.session_state.message_history,
                    (
                        "user",
                        [
                            {
                                "type": "text",
                                "text": user_input
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "auto"
                                },
                            }
                        ]
                    )
                ])
            #response = chain.invoke(user_input)
         
        if st.session_state.model_name ==  "command-r-plus":
            print("st.session_state.model_name=",st.session_state.model_name)
            print(user_input)
            prompt = ChatPromptTemplate.from_messages(
                [
                    *st.session_state.message_history,
                     #("user", f"{user_input}:{base64_image}"),  #ã‚„ã£ã±ã‚Šã ã‚
                     ("user", f"{user_input}")
                ]
            )
            
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            #stream = chain.stream(user_input,base64_image)
            
            stream = chain.stream({"user_input":user_input,"base64_image": base64_image})
            print("stream=",stream)
            #response = chain.invoke(user_input)
            
        else:
            print("st.session_state.model_name=",st.session_state.model_name)
            if st.session_state.input_img == "æœ‰":
                prompt = ChatPromptTemplate.from_messages(
                    [
                        *st.session_state.message_history,
                        (
                            "user",
                            [
                                {
                                    "type": "text",
                                    "text": user_input
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                                }
                            ],
                        ),
                    ]
                )
              
                output_parser = StrOutputParser()
                chain = prompt | st.session_state.llm | output_parser
                #stream = chain.stream(user_input,base64_image)
                stream = chain.stream({"user_input":user_input,"base64_image": base64_image})
                #print("stream=",stream)
                #response = chain.invoke(user_input)
            else:
                prompt = ChatPromptTemplate.from_messages(
                    [
                        *st.session_state.message_history,
                        (
                            "user",
                            [
                                {
                                    "type": "text",
                                    "text": user_input
                                }
                            ],
                        ),
                    ]
                )
                
            output_parser = StrOutputParser()
            chain = prompt | st.session_state.llm | output_parser
            if st.session_state.output_method == "éŸ³å£°":
                response = chain.invoke({"user_input":user_input})
                #speak(response)   #st.audio ok
                streaming_text_speak(response)
            else:    
                stream = chain.stream({"user_input":user_input})
            # LLMã®è¿”ç­”ã‚’è¡¨ç¤ºã™ã‚‹  Streaming
                with st.chat_message('ai'):  
                    response =st.write_stream(stream) 
                           
            print(f"{st.session_state.model_name}=",response)
 
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
            st.session_state.message_history.append(("user", user_input))
            st.session_state.message_history.append(("ai", response))
        
            return response
    except StopIteration:
        # StopIterationã®å‡¦ç†
        print("StopIterationãŒç™ºç”Ÿ")
        pass

    user_input = ""
    base64_image = ""
    frame = ""   

def main():
    #st.header("Real Time Speech-to-Text with_video")
    #ç”»é¢è¡¨ç¤º
    init_page()
    init_messages()
    #stã§ä½¿ã†å¤‰æ•°åˆæœŸè¨­å®š
    st.session_state.llm = select_model()
    st.session_state.input_method = ""
    st.session_state.user_input = ""
    st.session_state.result = ""
    st.session_state.frame = "" 
    st.session_state.text_output = ""
    st.session_state.audio_receiver_size = 4096

    col1, col2 ,col3= st.sidebar.columns(3)
     # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
    with col1:
        # å…¥åŠ›æ–¹æ³•ã®é¸æŠ
        input_method = st.sidebar.radio("å…¥åŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.input_method = input_method
    with col2:
        # ç”»åƒã«ã¤ã„ã¦ã®å•åˆã›æœ‰ç„¡ã®é¸æŠ
        input_img = st.sidebar.radio("  ã‚«ãƒ¡ãƒ©ç”»åƒå•åˆã›", ("æœ‰", "ç„¡"))
        st.session_state.input_img = input_img
    with col3:
        # å‡ºåŠ›æ–¹æ³•ã®é¸æŠ
        output_method = st.sidebar.radio("å‡ºåŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.output_method = output_method
    

    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º (ç¬¬2ç« ã‹ã‚‰å°‘ã—ä½ç½®ãŒå¤‰æ›´ã«ãªã£ã¦ã„ã‚‹ã®ã§æ³¨æ„)
    for role, message in st.session_state.get("message_history", []):
        st.chat_message(role).markdown(message)
    #ãƒ‡ãƒ¼ã‚¿åˆæœŸå€¤
    user_input = ""
    base64_image = ""
    frame = ""    
    app_sst_with_video() 

class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        self.frame = None
    def recv(self, frame):   
        self.frame = frame.to_ndarray(format="bgr24")
        return frame
        
async def process_audio(audio_data_bytes, sample_rate):
    temp_audio_file = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
    with wave.open(temp_audio_file, 'wb') as wf:
        wf.setnchannels(2)
        #wf.setnchannels(1)  # ãƒ¢ãƒãƒ©ãƒ«éŸ³å£°ã¨ã—ã¦è¨˜éŒ²ã€€NG
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio_data_bytes)
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ transcribe ã«æ¸¡ã™
    temp_audio_file_path = temp_audio_file.name 
    temp_audio_file.close()  
    # Whisperã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    model = whisper.load_model("small")  # ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¯é©å®œé¸æŠ
    #base:74M,small:244M,medium,large
    # éŸ³å£°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
    try:
        # Whisperã§éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
        result = model.transcribe(temp_audio_file_path, language="ja")  # æ—¥æœ¬èªæŒ‡å®š
        answer = result['text']
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        os.remove(temp_audio_file_path)
    
        
    # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
    if answer == "" :
        print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©º")
        #return None 
    elif "ã”è¦–è´" in answer or "ãŠç–²ã‚Œæ§˜" in answer:
        print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒã€Œã”è¦–è´ã€ã€ã¾ãŸã¯ã€ŒãŠç–²ã‚Œæ§˜ã€ã‚’å«ã‚€")
        #return None 
    else:
        print(answer)
        return answer

def save_audio(audio_segment: AudioSegment, base_filename: str) -> None:
    filename = f"{base_filename}_{int(time.time())}.wav"
    audio_segment.export(filename, format="wav")

def transcribe(audio_segment: AudioSegment, debug: bool = False) -> str:
    """
    OpenAIã®Whisper ASRã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ–‡å­—èµ·ã“ã—ã—ã¾ã™ã€‚
    å¼•æ•°:
        audio_segment (AudioSegment): æ–‡å­—èµ·ã“ã—ã™ã‚‹éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã€‚
        debug (bool): Trueã®å ´åˆã€ãƒ‡ãƒãƒƒã‚°ç›®çš„ã§éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã™ã€‚
    æˆ»ã‚Šå€¤:
        str: æ–‡å­—èµ·ã“ã—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€‚
    """
    if debug:
        save_audio(audio_segment, "debug_audio")
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
        audio_segment.export(tmpfile.name, format="wav")
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
        audio = whisper.load_audio(tmpfile.name)
        audio = whisper.pad_or_trim(audio)
        # Whisperã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        model = whisper.load_model("small")  # ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¯é©å®œé¸æŠ
        #base:74M,small:244M,medium,large
        # éŸ³å£°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        result = model.transcribe(audio, language="ja")  # æ—¥æœ¬èªã‚’æŒ‡å®š
        answer = result['text']
        
        # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
        if len(answer) < 5 or "ã”è¦–è´" in answer or "ãŠç–²ã‚Œæ§˜" in answer:
            #print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©º")
            #print("transcribeãƒ«ãƒ¼ãƒãƒ³ã®text(answer)=",answer)
            return ""
        elif "è¦‹ã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†" in answer or "ã¯ã£ã¯ã£ã¯" in answer:
            #print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒã€Œã”è¦–è´ã€ã€ã¾ãŸã¯ã€ŒãŠç–²ã‚Œæ§˜ã€ã‚’å«ã‚€")
            return "" 
        elif "ã‚“ã‚“ã‚“ã‚“ã‚“ã‚“" in answer :
            #print("ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒã€Œã”è¦–è´ã€ã€ã¾ãŸã¯ã€ŒãŠç–²ã‚Œæ§˜ã€ã‚’å«ã‚€")
            return "" 
    tmpfile.close()  
    os.remove(tmpfile.name)
    print("transcribeãƒ«ãƒ¼ãƒãƒ³ã®text(answer)=",answer)
    st.session_state.text_output = answer
    return answer

def frame_energy(frame):
    samples=[]
    # ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿ã‚’numpyé…åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã¿
    samples = np.frombuffer(frame.to_ndarray().tobytes(), dtype=np.int16)
    # NaNã€æ­£ã®ç„¡é™å¤§ã€è² ã®ç„¡é™å¤§ã‚’0ã«ç½®æ›
    samples = np.nan_to_num(samples, nan=0.0, posinf=0.0, neginf=0.0)
    # é…åˆ—ã®é•·ã•ãŒ0ã®å ´åˆã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’0ã¨ã—ã¦è¿”ã™
    if len(samples) == 0: 
        return 0.0
    # è² ã®å€¤ã‚’çµ¶å¯¾å€¤ã«å¤‰æ›ã—ã¦å‡¦ç† 
    samples = np.abs(samples)
    try:
        #print("samples=",samples)
        energy = np.sqrt(np.mean(samples**2))
        #print("energy=",energy)  #50-90
        return energy  
    except Exception as e:
        #print(f"Error exporting audio: {e}")
        return 0.0

def frame_amplitude(audio_frame):
    samples = np.frombuffer(audio_frame.to_ndarray().tobytes(), dtype=np.int16)
    max_amplitude = np.max(np.abs(samples))
    #print("max_amplitude=",max_amplitude)
    return max_amplitude 

def process_audio_frames(audio_frames, sound_chunk, silence_frames, energy_threshold, amp_threshold):
    """
    éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é †æ¬¡å‡¦ç†ã—ã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã€‚
    ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ãŒä¸€å®šæ•°ä»¥ä¸Šç¶šã„ãŸå ´åˆã€ç„¡éŸ³åŒºé–“ã¨ã—ã¦å‡¦ç†ã—ã€å¾Œç¶šã®å‡¦ç†ï¼ˆä¾‹ãˆã°ã€éŸ³å£°èªè­˜ã®ãƒˆãƒªã‚¬ãƒ¼ï¼‰ã«å½¹ç«‹ã¦ã¾ã™ã€‚
    ã“ã®å‡¦ç†ã«ã‚ˆã‚Šã€ç„¡éŸ³ã‚„éŸ³å£°ã®æœ‰ç„¡ã‚’æ­£ç¢ºã«æ¤œå‡ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

    éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        audio_frames (list[VideoTransformerBase.Frame]): å‡¦ç†ã™ã‚‹éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã€‚
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        silence_frames (int): ç¾åœ¨ã®ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        energy_threshold (int): ç„¡éŸ³æ¤œå‡ºã«ä½¿ç”¨ã™ã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ã—ãã„å€¤ã€‚
        amp_threshold:ç„¡éŸ³æ¤œå‡ºã«ä½¿ç”¨ã™ã‚‹æœ€å¤§æŒ¯å¹…ã—ãã„å€¤ã€‚
    æˆ»ã‚Šå€¤ï¼š
        tuple[AudioSegment, int]: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã¨ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        
    """
        
    for audio_frame in audio_frames:
        sound_chunk = add_frame_to_chunk(audio_frame, sound_chunk)

        energy = frame_energy(audio_frame)
        amplitude = frame_amplitude(audio_frame)

        if energy < energy_threshold or amplitude < amp_threshold:
            silence_frames += 1 
            #ç„¡éŸ³ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆã¯æœ€å¤§æŒ¯å¹…ãŒã—ãã„å€¤ä»¥ä¸‹ã§ã‚ã‚‹å ´åˆã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã‚’1ã¤å¢—ã‚„ã—ã¾ã™ã€‚
        else:
            silence_frames = 0 
            #ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆã¯æœ€å¤§æŒ¯å¹…ãŒã—ãã„å€¤ã‚’è¶…ãˆã‚‹å ´åˆã€ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦0ã«ã—ã¾ã™ã€‚

    return sound_chunk, silence_frames,energy,amplitude

def add_frame_to_chunk(audio_frame, sound_chunk):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        audio_frame (VideoTransformerBase.Frame): è¿½åŠ ã™ã‚‹ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
    æˆ»ã‚Šå€¤ï¼š
        AudioSegment: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
    """
    sound = pydub.AudioSegment(
        data=audio_frame.to_ndarray().tobytes(),
        sample_width=audio_frame.format.bytes,
        frame_rate=audio_frame.sample_rate,
        channels=len(audio_frame.layout.channels),
    )
    sound_chunk += sound
    return sound_chunk

def handle_silence(sound_chunk, silence_frames, silence_frames_threshold):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã®ç„¡éŸ³ã‚’å‡¦ç†ã—ã¾ã™ã€‚ 
    å¼•æ•°ï¼š
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        silence_frames (int): ç¾åœ¨ã®ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
        silence_frames_threshold (int): ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã—ãã„å€¤ã€‚
        text_output (st.empty): Streamlitã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    æˆ»ã‚Šå€¤ï¼š
        tuple[AudioSegment, int]: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã¨ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•°ã€‚
   
    """
    if silence_frames >= silence_frames_threshold: 
        #ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒ100ä»¥ä¸Šã®æ™‚ã€éŸ³å£°ã®é€”åˆ‡ã‚Œï¼ˆé–“éš”ï¼‰ã¨ã—ã¦æ‰±ã†
        if len(sound_chunk) > 0:
            #ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒé€£ç¶šã—ãŸã‚‰ã€éŸ³å£°ã®é€”åˆ‡ã‚Œã¨ã—ã¦ã€ãã“ã¾ã§ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã¦ã„ã‚‹
            text = transcribe(sound_chunk)
            #text_output.write(text)
            #print("handle_silenceãƒ«ãƒ¼ãƒãƒ³ã®text=",text)
            #print("ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ å†…ã®ç„¡éŸ³æ™‚ã®å¿œç­”=",text)
            
            sound_chunk = pydub.AudioSegment.empty()
            silence_frames = 0
    return sound_chunk, silence_frames

def handle_queue_empty(sound_chunk, text_output):
    """
    ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚­ãƒ¥ãƒ¼ãŒç©ºã®å ´åˆã®å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚
    å¼•æ•°:
        sound_chunk (AudioSegment): ç¾åœ¨ã®ã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
        text_output (st.empty): Streamlitã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    æˆ»ã‚Šå€¤:
        AudioSegment: æ›´æ–°ã•ã‚ŒãŸã‚µã‚¦ãƒ³ãƒ‰ãƒãƒ£ãƒ³ã‚¯ã€‚
    """
    if len(sound_chunk) > 0:
        text = transcribe(sound_chunk)
        #text_output.write(text)
        #st.session_state.text_output = text
        sound_chunk = pydub.AudioSegment.empty()
    return sound_chunk

# ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ¥ãƒ¼ã®ä½œæˆ 
# å›ºå®šã‚µã‚¤ã‚ºã®ã‚­ãƒ¥ãƒ¼ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®ã‚‚ã®
class CustomQueue: 
    def __init__(self, maxsize): 
        self.queue = deque(maxlen=maxsize)
        #maxsize ã¨ã„ã†å¼•æ•°ã‚’å—ã‘å–ã‚Šã€ãã®ã‚µã‚¤ã‚ºã®dequeï¼ˆåŒæ–¹å‘ã‚­ãƒ¥ãƒ¼ï¼‰ã‚’ä½œæˆã—ã¾ã™ã€‚
        # dequeã¯Pythonã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ä¸€ç¨®ã§ã€ãƒªã‚¹ãƒˆã®ã‚ˆã†ãªã‚‚ã®ã§ã€
        # maxlenã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã‚­ãƒ¥ãƒ¼ã®æœ€å¤§ã‚µã‚¤ã‚ºã‚’è¨­å®šã§ãã¾ã™ã€‚è‡ªå‹•çš„ã«å¤ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’å‰Šé™¤ã—ã¾ã™ã€‚ 
    def put(self, item): 
        #print("len(self.queue)=",len(self.queue))
        # maxlenãŒNoneã§ãªã„ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰æ¯”è¼ƒ
        if self.queue.maxlen is not None and len(self.queue) >= self.queue.maxlen:
            print("Queue overflow. Oldest item will be discarded.") 
        self.queue.append(item)
        #ã‚­ãƒ¥ãƒ¼ã«æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¿½åŠ  
    def get(self): 
        #ã‚­ãƒ¥ãƒ¼ã‹ã‚‰æœ€ã‚‚å¤ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–ã‚Šå‡ºã—ã¦è¿”ã—ã¾ã™ã€‚
        #ã‚­ãƒ¥ãƒ¼ãŒç©ºã§ãªã„å ´åˆã¯popleftãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã£ã¦æœ€ã‚‚å¤ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–ã‚Šå‡ºã—ã€
        # ã‚­ãƒ¥ãƒ¼ãŒç©ºã®å ´åˆã¯queue.Emptyä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã¾ã™ã€‚
        if len(self.queue) > 0: 
            return self.queue.popleft() 
        else: 
            raise queue.Empty 
    def qsize(self): 
        return len(self.queue)         


def app_sst_with_video():
    text_input = ""
    frames_deque_lock = threading.Lock()
    frames_deque: deque = deque([])

    async def queued_audio_frames_callback(
        frames: List[av.AudioFrame],
    ) -> av.AudioFrame:
        with frames_deque_lock:
            frames_deque.extend(frames)

        # Return empty frames to be silent.
        new_frames = []
        for frame in frames:
            input_array = frame.to_ndarray()
            new_frame = av.AudioFrame.from_ndarray(
                np.zeros(input_array.shape, dtype=input_array.dtype),
                layout=frame.layout.name,
            )
            new_frame.sample_rate = frame.sample_rate
            new_frames.append(new_frame)

        return new_frames
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¡¨ç¤º
    with st.sidebar:
        st.header("Webcam Stream")
        webrtc_ctx = webrtc_streamer(
            key="speech-to-text-w-video",
            desired_playing_state=True, 
            mode=WebRtcMode.SENDRECV, #.SENDONLY,  #
            #audio_receiver_size=2048,  #1024ã€€#512 #ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4
            #å°ã•ã„ã¨Queue overflow. Consider to set receiver size bigger. Current size is 1024.
            queued_audio_frames_callback=queued_audio_frames_callback,
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            media_stream_constraints={"video": True, "audio": True},
            video_processor_factory=VideoTransformer,  
        )
    
    #status_indicator.write("Loading...")
    cap_title = st.sidebar.empty()    
    cap_image = st.sidebar.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ 
    status_indicator = st.sidebar.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
    
    ###################################################################
    #éŸ³å£°å…¥åŠ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ãŸå…¥åŠ›ï¼‰ã®å¯¾è©±ãƒ«ãƒ¼ãƒ—
    #print("Before_st.session_state.input_method=",st.session_state.input_method)
    if st.session_state.input_method == "éŸ³å£°": 

        if webrtc_ctx is None:
            st.error("WebRTC context is not initialized. Please check your setup.")
        if webrtc_ctx and webrtc_ctx.audio_receiver is None:
            st.error("Audio receiver is not available. Please check the WebRTC mode.")
        if webrtc_ctx and webrtc_ctx.state.playing and webrtc_ctx.audio_receiver:
            audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=st.session_state.timeout)
            #st.write(f"Received {len(audio_frames)} audio frames.")
        else:
            st.info("Waiting for audio stream to initialize...")
        if not webrtc_ctx.state.playing:
            return    
        audio_receiver_size = st.sidebar.slider(
        "éŸ³å£°å—ä¿¡å®¹é‡ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4096:", 
        min_value=512, max_value=4096, value=4096, step=512
        )
        st.session_state.audio_receiver_size =audio_receiver_size
        # ç„¡éŸ³ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã®é–¾å€¤    
        energy_threshold = st.sidebar.slider(
        "ç„¡éŸ³æœ€å¤§ã‚¨ãƒãƒ«ã‚®ãƒ¼ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ400:", 
        min_value=100, max_value=900, value=400, step=100
        )
        energy_indicator = st.sidebar.empty() 

        amp_threshold = st.sidebar.slider(
            "ç„¡éŸ³æœ€å¤§æŒ¯å¹…ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ800:", 
            min_value=200, max_value=1600, value=800, step=100
            )
        amp_indicator = st.sidebar.empty() 

        silence_frames_threshold = st.sidebar.slider(
            "éŸ³å£°é€”åˆ‡ã‚Œæ¤œå‡ºå¹…ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100:", 
            min_value=0, max_value=200, value=100, step=10
            )
        #60ãŒBest,ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100
        st.session_state.timeout = st.sidebar.slider(
            "éŸ³å£°æ¤œå‡ºã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3ç§’:", 
            min_value=1, max_value=3, value=1, step=1
            )
        st.write("ğŸ¤–ä½•ã‹è©±ã—ã¦!")
        status_indicator = st.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        status_indicator.write("éŸ³å£°èªè­˜å‹•ä½œä¸­...")
        text_output = st.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        #st.sidebar.header("Capture Image")
        #cap_image = st.sidebar.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        
        #stã§ä½¿ã†å¤‰æ•°åˆæœŸè¨­å®š
        st.session_state.energy_threshold = energy_threshold
        st.session_state.amp_threshold = amp_threshold
        st.session_state.silence_frames_threshold = silence_frames_threshold
        sound_chunk = pydub.AudioSegment.empty()
        silence_frames = 0
        #CustomQueue ã‚¯ãƒ©ã‚¹ã‚’ä½¿ã£ã¦ã‚­ãƒ¥ãƒ¼ã‚’ä½œæˆã—ã€
        # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã«ã¯å¤ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’å‰Šé™¤ã™ã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚
        # ã“ã‚Œã«ã‚ˆã‚Šã€ã‚­ãƒ¥ãƒ¼ãŒã„ã£ã±ã„ã«ãªã£ã¦ã‚‚æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
        # CustomQueueã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ 
        #audio_queue = CustomQueue(MAX_QUEUE_SIZE)
        audio_queue = CustomQueue(st.session_state.audio_receiver_size)
        while True:
            if webrtc_ctx.audio_receiver:

                #handle_queue_overflow(webrtc_ctx.audio_receiver,st.session_state.audio_receiver_size)
                
                try:
                    audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=st.session_state.timeout)
                    #print("len(audio_frames)=",len(audio_frames))
                    
                    for frame in audio_frames:
                        audio_queue.put(frame)    
                except queue.Empty:
                    status_indicator.write("No frame arrived.")
                    sound_chunk = handle_queue_empty(sound_chunk, text_output)
                    continue
                #print("len(audio_frames)=",len(audio_frames))    
                #sound_chunk, silence_frames ,energy,amplitude= process_audio_frames(audio_frames, sound_chunk, silence_frames, st.session_state.energy_threshold,st.session_state.amp_threshold)
                #sound_chunk, silence_frames = handle_silence(sound_chunk, silence_frames, st.session_state.silence_frames_threshold)
                try: 
                    while audio_queue.qsize() > 0: 
                        frame = audio_queue.get() 
                        sound_chunk, silence_frames, energy, amplitude = process_audio_frames([frame], sound_chunk, silence_frames, st.session_state.energy_threshold, st.session_state.amp_threshold) 
                        sound_chunk, silence_frames = handle_silence(sound_chunk, silence_frames, st.session_state.silence_frames_threshold) 
                        #print("len(sound_chunk) =",len(sound_chunk),"audio_queue.qsize() =",audio_queue.qsize() )
                        #print("audio_queue.qsize() =",audio_queue.qsize())    
                        # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦å‡¦ç† 
                        try:
                            energy = 0.0 if np.isnan(energy) else energy
                            energy = round(energy)
                        except Exception as e:
                            #print(f"Error exporting round(energy): {e}")
                            energy = 0

                        energy_indicator.write(f"éŸ³å£°ã‚¨ãƒãƒ«ã‚®ãƒ¼={energy}")
                        amp_indicator.write(f"éŸ³å£°æŒ¯å¹…={amplitude}")
                        if len(st.session_state.text_output) > 4 :
                            print("st.session_state.text_output=",st.session_state.text_output)    
                            text_input =  st.session_state.text_output 
                            st.session_state.text_output = ""
                            st.write(f"Received {len(audio_frames)} audio frames.")
                            #ã“ã‚Œä»¥é™ã¯ã€éŸ³å£°å…¥åŠ›ã€ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›å…±é€šã®å‡¦ç†ã¸
                            qa(text_input,webrtc_ctx,cap_title,cap_image)
                            st.write(f"ğŸ¤–ä½•ã‹è©±ã—ã¦!")  
                            text_input = ""
                except queue.Empty: 
                    status_indicator.write("Queue is empty.")
               
            else:
                status_indicator.write("éŸ³å£°èªè­˜åœæ­¢")
                break

    ################################################################### 
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®å ´åˆ
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    if st.session_state.input_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
        button_input = ""
        # 4ã¤ã®åˆ—ã‚’ä½œæˆ
        col1, col2, col3, col4 = st.columns(4)
        # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
        with col1:
            if st.button("ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
                button_input = "ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"
        with col2:
            if st.button("å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"):
                button_input = "å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"
        with col3:
            if st.button("ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"):
                button_input = "ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"
        with col4:
            if st.button("äººç”Ÿã®æ„ç¾©ã¯ï¼Ÿ"):
                button_input = "äººç”Ÿã®æ„ç¾©ï¼Ÿ"
        col5, col6, col7, col8 = st.columns(4)
        with col5:
            if st.button("æ—¥æœ¬ã®æ‚ªã„ã¨ã“ã‚ã¯ï¼Ÿ"):
                button_input = "æ—¥æœ¬ã®æ‚ªã„ã¨ã“ã‚ã¯ï¼Ÿ"
        with col6:
            if st.button("å–„æ‚ªã¯ä½•ã§æ±ºã¾ã‚Šã¾ã™ã‹ï¼Ÿ"):
                button_input = "å–„æ‚ªã¯ä½•ã§æ±ºã¾ã‚Šã¾ã™ã‹ï¼Ÿ"
        with col7:
            if st.button("å°æ¾å¸‚ã®ãŠã„ã—ã„æ–™ç†åº—ã¯ï¼Ÿ"):
                button_input = "å°æ¾å¸‚ã®ãŠã„ã—ã„æ–™ç†åº—ã¯ï¼Ÿ"
        with col8:
            if st.button("ä»Šæ—¥ã®æ–™ç†ã¯ãªã«ãŒã„ã„ã‹ãª"):
                button_input = "ä»Šæ—¥ã®æ–™ç†ã¯ãªã«ãŒã„ã„ã‹ãª"
        if button_input !="":
            st.session_state.user_input=button_input

        text_input =st.chat_input("ğŸ¤—ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ã“ã“ã«å…¥åŠ›ã—ã¦ã­ï¼") #,key=st.session_state.text_input)
        #text_input = st.text_input("ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å…¥åŠ›ã—ã¦ãã ã•ã„:", key=st.session_state.text_input) 
        if button_input:
            text_input = button_input
        if text_input:
            qa(text_input,webrtc_ctx,cap_title,cap_image)

################################################################### 
def qa(text_input,webrtc_ctx,cap_title,cap_image):
     # æœ«å°¾ã®ç©ºç™½ã®æ•°ã‚’ç¢ºèª
    trailing_spaces = len(text_input) - len(text_input.rstrip())
    print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæœ«å°¾ã®ç©ºç™½ã®æ•°: {trailing_spaces}")
    # æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    cleaned_text = text_input.rstrip()
    #print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæœ«å°¾ã®ç©ºç™½ã‚’é™¤å»ã—ãŸæ–‡å­—åˆ—: '{cleaned_text}'")
    with st.chat_message('user'):   
        st.write(cleaned_text) 
    # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
    cap = None 
    if st.session_state.input_img == "æœ‰":
        # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
        #ç¾åœ¨ã®ç”»åƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹
        #ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ¼ç”»åƒå…¥åŠ›
        if webrtc_ctx.video_transformer:  
            cap = webrtc_ctx.video_transformer.frame
        if cap is not None :
            #st.sidebar.header("Capture Image") 
            cap_title.header("Capture Image")     
            cap_image.image(cap, channels="BGR")
            # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
    # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
    with st.spinner("Querying LLM..."):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        st.session_state.result= ""
        result = loop.run_until_complete(query_llm(cleaned_text,cap))
        st.session_state.result = result
    result = ""
    text_input="" 
###################################################################      
if __name__ == "__main__":
    
    main()
